{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Load and Analyze the UMAP cooridnates Excel files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libs\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPIC v1 and EPIC v2 Samples that contain the same DNA \n",
    "parings = {\"205756360010_R01C01\":\"207097420059_R02C01\",\n",
    "           \"205705530027_R06C01\":\"207097420059_R03C01\",\n",
    "           \"205707890147_R04C01\":\"207097420059_R04C01\",\n",
    "           \"205751550059_R06C01\":\"207097420059_R05C01\",\n",
    "           \"205800610010_R01C01\":\"207097420059_R06C01\",\n",
    "           \"205800610132_R07C01\":\"207097420059_R07C01\",\n",
    "           \"205800610140_R04C01\":\"207097420059_R08C01\",\n",
    "           \"205800610140_R08C01\":\"207107850059_R01C01\",\n",
    "           \"205800610140_R07C01\":\"207107850059_R02C01\",\n",
    "           \"205800600144_R04C01\":\"207107850059_R03C01\",\n",
    "           \"205800600144_R01C01\":\"207107850059_R04C01\",\n",
    "           \"205800600144_R03C01\":\"207107850059_R05C01\",\n",
    "           \"205809360101_R05C01\":\"207107850059_R06C01\",\n",
    "           \"205921770008_R02C01\":\"207107850059_R07C01\",\n",
    "           \"205982890034_R02C01\":\"207107850059_R08C01\",\n",
    "           \"205982890034_R07C01\":\"207107850085_R01C01\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of EPIC v1 and EPIC v2 Samples \n",
    "listv1 = list(parings.keys())\n",
    "print(listv1)\n",
    "listv2 = list(parings.values())\n",
    "print(listv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest reference Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_NN_MethCLass(df):\n",
    "    points = np.array(df[[\"UMAP 0\", \"UMAP 1\"]])\n",
    "    length = len(points)\n",
    "    for i, point in enumerate(points[-32:]):\n",
    "        distances = np.linalg.norm(points[:-32]-point, axis=1)\n",
    "        min_distance_index = np.argmin(distances)\n",
    "        df.at[length-32+i, \"MethText\"] = df.at[min_distance_index, \"MethClass\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN or K nearest neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nearest_Neighbor2(df, k = 15):\n",
    "    points = np.array(df[[\"UMAP 0\", \"UMAP 1\"]])\n",
    "    length = len(points)\n",
    "    for i, point in enumerate(points[-32:]):\n",
    "        distances = np.linalg.norm(points[:-32]-point, axis=1)\n",
    "        closest_indices = np.argsort(distances)[:k]\n",
    "        closest_samples = df.iloc[closest_indices]\n",
    "        d = defaultdict(lambda:1)\n",
    "        for row in closest_samples.itertuples(index=False):\n",
    "            if row.MethClass in d.keys():\n",
    "                d[row.MethClass] += 1\n",
    "            else:\n",
    "                d[row.MethClass]\n",
    "\n",
    "        max_value = max(d.values())\n",
    "        list_of_max_classes = [key for key, value in d.items() if value == max_value]\n",
    "        min = np.inf\n",
    "        closes_class = \"\"\n",
    "        if len(list_of_max_classes) > 1:\n",
    "            for entry in list_of_max_classes: \n",
    "                indx= df.index[df[\"MethClass\"]==entry].tolist()\n",
    "                mean = np.mean(points[indx])\n",
    "                if mean < min:\n",
    "                    min = mean\n",
    "                    closes_class = entry\n",
    "            df.at[length-32+i, \"MethText\"] = closes_class\n",
    "        else:\n",
    "            df.at[length-32+i, \"MethText\"] = list_of_max_classes[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tumor_centroids(df):\n",
    "    tumor_class_centroids = {}\n",
    "    for i in df[\"MethClass\"].unique():\n",
    "        temp = df[df[\"MethClass\"]==i]\n",
    "        tumor_class_centroids[i] = np.mean(np.array(temp[[\"UMAP 0\", \"UMAP 1\"]]), axis=0)\n",
    "    return tumor_class_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_cluster_MethCLass(df):\n",
    "    tumor_class_centroids = get_tumor_centroids(df)\n",
    "    length = len(df.index)\n",
    "    for i, point in enumerate(np.array(df[-32:][[\"UMAP 0\", \"UMAP 1\"]])):\n",
    "        distances = np.linalg.norm(np.array(list(tumor_class_centroids.values()))-point, axis=1) \n",
    "        min_distance_index = np.argmin(distances)\n",
    "        tumor_class = list(tumor_class_centroids.keys())[min_distance_index]\n",
    "        df.at[length-32+i, \"MethText\"] = tumor_class\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max Distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_dist(df):\n",
    "    df2 = df.iloc[:-32]\n",
    "    df2.sort_values(\"MethClass\")\n",
    "    points = np.array(df2[[\"UMAP 0\", \"UMAP 1\"]])\n",
    "    length = len(df)\n",
    "    dict1 = {}\n",
    "    for meth_class in df2[\"MethClass\"].unique():\n",
    "    # Get indices where the unique entry appears\n",
    "        indices = df2[df2[\"MethClass\"] == meth_class].index.tolist()\n",
    "    # Store in the dictionary\n",
    "        dict1[meth_class] = indices\n",
    "    for i, point in enumerate(np.array(df.iloc[-32:][[\"UMAP 0\", \"UMAP 1\"]])):\n",
    "        dict2 = {}\n",
    "        distances = np.linalg.norm(points-point, axis=1)\n",
    "        for key in dict1:\n",
    "            dict2[key] = np.max(distances[dict1[key]], axis=0)\n",
    "        smallest_key = min(dict2, key=dict2.get)\n",
    "        df.at[length-32+i, \"MethText\"] = smallest_key\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unneeded columnsplus compare Annotations/Classifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    df1 = df.iloc[-32:-16]\n",
    "    df2 = df.iloc[-16:]\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_df(df1, df2, list1, list2):\n",
    "    df1.sort_values(by=\"SentrixID\", key=lambda column: column.map(lambda e: list1.index(e)), inplace=True)\n",
    "    df2.sort_values(by=\"SentrixID\", key=lambda column: column.map(lambda e: list2.index(e)), inplace=True)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mmeth_text(dfv1, dfv2):\n",
    "    dfv1.reset_index(drop=True, inplace=True)\n",
    "    dfv2.reset_index(drop=True, inplace=True)\n",
    "    dfv1[\"SentrixID_V2\"] = dfv2[\"SentrixID\"]\n",
    "    dfv1[\"MethText_V2\"] = dfv2[\"MethText\"]\n",
    "    return dfv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(df):\n",
    "    df[\"Comaprison\"] = (df[\"MethText\"]==df[\"MethText_V2\"]).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_split_order_recombine_compare(df):\n",
    "    a, b = split_df(strip_cols(df))\n",
    "    c, d = order_df(a, b, listv1, listv2)\n",
    "    df1 = combine_mmeth_text(c,d)\n",
    "    df2 = compare(df1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and analyzing all files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_files_nearest_ref_sample(path):\n",
    "    files = os.listdir(path)\n",
    "    excel_files = [file for file in files if file.endswith('.xlsx')]\n",
    "    correct_ids = []\n",
    "    classifications = pd.DataFrame()\n",
    "    for file in excel_files:\n",
    "        data = pd.read_excel(os.path.join(path,file))\n",
    "        data= data.drop([\"Unnamed: 0\"],axis=1)  \n",
    "        data = annotate_NN_MethCLass(data)\n",
    "        classifications[\"SentrixID\"] = data[-32:][\"SentrixID\"]\n",
    "        classifications[file] = data[-32:][\"MethText\"]\n",
    "        data = strip_split_order_recombine_compare(data)\n",
    "        correct_ids.append(data[\"Comaprison\"].sum())\n",
    "    median = np.median(correct_ids)\n",
    "    std = np.std(correct_ids)\n",
    "    print(correct_ids)\n",
    "    print(f\"The Median is : {np.median(correct_ids)}\")\n",
    "    print(f\"The Std is : {np.std(correct_ids)}\")\n",
    "    return [correct_ids, median, std, classifications]\n",
    "\n",
    "def analyze_all_files_nearest_cluster(path):\n",
    "    files = os.listdir(path)\n",
    "    excel_files = [file for file in files if file.endswith('.xlsx')]\n",
    "    correct_ids = []\n",
    "    classifications = pd.DataFrame()\n",
    "    for file in excel_files:\n",
    "        data = pd.read_excel(os.path.join(path,file))\n",
    "        data= data.drop([\"Unnamed: 0\"],axis=1)  \n",
    "        data = annotate_cluster_MethCLass(data)\n",
    "        classifications[\"SentrixID\"] = data[-32:][\"SentrixID\"]\n",
    "        classifications[file] = data[-32:][\"MethText\"]\n",
    "        data = strip_split_order_recombine_compare(data)\n",
    "        correct_ids.append(data[\"Comaprison\"].sum())\n",
    "    median = np.median(correct_ids)\n",
    "    std = np.std(correct_ids)\n",
    "    print(correct_ids)\n",
    "    print(f\"The Median is : {np.median(correct_ids)}\")\n",
    "    print(f\"The Std is : {np.std(correct_ids)}\")\n",
    "    return [correct_ids, median, std, classifications]\n",
    "\n",
    "def analyze_all_files_KNN2(path):\n",
    "    files = os.listdir(path)\n",
    "    excel_files = [file for file in files if file.endswith('.xlsx')]\n",
    "    correct_ids = []\n",
    "    classifications = pd.DataFrame()\n",
    "    for file in excel_files:\n",
    "        data = pd.read_excel(os.path.join(path,file))\n",
    "        data= data.drop([\"Unnamed: 0\"],axis=1)  \n",
    "        data = Nearest_Neighbor2(data)\n",
    "        classifications[\"SentrixID\"] = data[-32:][\"SentrixID\"]\n",
    "        classifications[file] = data[-32:][\"MethText\"]\n",
    "        data = strip_split_order_recombine_compare(data)\n",
    "        correct_ids.append(data[\"Comaprison\"].sum())\n",
    "    median = np.median(correct_ids)\n",
    "    std = np.std(correct_ids)\n",
    "    print(correct_ids)\n",
    "    print(f\"The Median is : {np.median(correct_ids)}\")\n",
    "    print(f\"The Std is : {np.std(correct_ids)}\")\n",
    "    return [correct_ids, median, std, classifications]\n",
    "\n",
    "def analyze_all_files_min_max_dist(path):\n",
    "    files = os.listdir(path)\n",
    "    excel_files = [file for file in files if file.endswith('.xlsx')]\n",
    "    correct_ids = []\n",
    "    classifications = pd.DataFrame()\n",
    "    for file in excel_files:\n",
    "        data = pd.read_excel(os.path.join(path,file))\n",
    "        data= data.drop([\"Unnamed: 0\"],axis=1)  \n",
    "        data = get_min_max_dist(data)\n",
    "        classifications[\"SentrixID\"] = data[-32:][\"SentrixID\"]\n",
    "        classifications[file] = data[-32:][\"MethText\"]\n",
    "        data = strip_split_order_recombine_compare(data)\n",
    "        correct_ids.append(data[\"Comaprison\"].sum())\n",
    "    median = np.median(correct_ids)\n",
    "    std = np.std(correct_ids)\n",
    "    print(correct_ids)\n",
    "    print(f\"The Median is : {np.median(correct_ids)}\")\n",
    "    print(f\"The Std is : {np.std(correct_ids)}\")\n",
    "    return [correct_ids, median, std, classifications]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
